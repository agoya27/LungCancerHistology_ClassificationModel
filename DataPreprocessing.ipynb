{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoXPHhFV3txn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sklearn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input Image Data of size 25000 images is stroes in my Google Drive. Unzip the Input Data file here.\n",
        "\n",
        "!unzip /content/drive/MyDrive/CancerImage.zip"
      ],
      "metadata": {
        "id": "2Jaw2GRT30Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained ResNet model which is used for Image Embedding\n",
        "model = models.resnet50(pretrained=True)\n",
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "gaV2X1o730dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess each Image before create image embeddings using Resnet50 Model\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_data = torch.empty(25000, 1000)\n",
        "input_count = 0"
      ],
      "metadata": {
        "id": "oVe-HnHc30pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Load the Images from Colon_aca Folder\n",
        "#2) Preprocess Each Image using the proprocess objects defined earlier\n",
        "#3) Create Image embedding using Resnet50 Model\n",
        "#4) Count the number of Images for Colan_aca\n",
        "\n",
        "for filename in os.listdir('/content/Cancer Image Classification/lung_colon_image_set/colon_image_sets/colon_aca'):\n",
        "    if filename.endswith('.jpeg'):\n",
        "        img_path = os.path.join('/content/Cancer Image Classification/lung_colon_image_set/colon_image_sets/colon_aca', filename)\n",
        "        with open(img_path, 'rb') as f:\n",
        "            im_bytes = f.read()\n",
        "            img = Image.open(BytesIO(im_bytes))\n",
        "            img_t = preprocess(img)\n",
        "            batch_t = torch.unsqueeze(img_t, 0)\n",
        "            with torch.no_grad():\n",
        "                features = model(batch_t)\n",
        "                # Flatten the features for easier use\n",
        "                embeddings = torch.flatten(features)\n",
        "                input_data[input_count] = embeddings\n",
        "                input_count += 1\n",
        "\n",
        "#input_count = 5048 (0->5047)"
      ],
      "metadata": {
        "id": "Zw_CyADV30wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Load the Images from Colon_n Folder\n",
        "#2) Preprocess Each Image using the proprocess objects defined earlier\n",
        "#3) Create Image embedding using Resnet50 Model\n",
        "#4) Count the number of Images for Colan_n\n",
        "\n",
        "for filename in os.listdir('/content/Cancer Image Classification/lung_colon_image_set/colon_image_sets/colon_n'):\n",
        "    if filename.endswith('.jpeg'):\n",
        "        img_path = os.path.join('/content/Cancer Image Classification/lung_colon_image_set/colon_image_sets/colon_n', filename)\n",
        "        with open(img_path, 'rb') as f:\n",
        "            im_bytes = f.read()\n",
        "            img = Image.open(BytesIO(im_bytes))\n",
        "            img_t = preprocess(img)\n",
        "            batch_t = torch.unsqueeze(img_t, 0)\n",
        "            with torch.no_grad():\n",
        "                features = model(batch_t)\n",
        "                # Flatten the features for easier use\n",
        "                embeddings = torch.flatten(features)\n",
        "                input_data[input_count] = embeddings\n",
        "                input_count += 1\n",
        "#input_count = 10048 (5048 -> 10047)"
      ],
      "metadata": {
        "id": "8zKYEY815SQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Load the Images from lung_aca Folder\n",
        "#2) Preprocess Each Image using the proprocess objects defined earlier\n",
        "#3) Create Image embedding using Resnet50 Model\n",
        "#4) Count the number of Images for lung_aca\n",
        "\n",
        "for filename in os.listdir('/content/Cancer Image Classification/lung_colon_image_set/lung_image_sets/lung_aca'):\n",
        "    if filename.endswith('.jpeg'):\n",
        "        img_path = os.path.join('/content/Cancer Image Classification/lung_colon_image_set/lung_image_sets/lung_aca', filename)\n",
        "        with open(img_path, 'rb') as f:\n",
        "            im_bytes = f.read()\n",
        "            img = Image.open(BytesIO(im_bytes))\n",
        "            img_t = preprocess(img)\n",
        "            batch_t = torch.unsqueeze(img_t, 0)\n",
        "            with torch.no_grad():\n",
        "                features = model(batch_t)\n",
        "                # Flatten the features for easier use\n",
        "                embeddings = torch.flatten(features)\n",
        "                input_data[input_count] = embeddings\n",
        "                input_count += 1\n",
        "#input_count = 15048 (10048 -> 15047)"
      ],
      "metadata": {
        "id": "5vRGs6R65SZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Load the Images from lung_n Folder\n",
        "#2) Preprocess Each Image using the proprocess objects defined earlier\n",
        "#3) Create Image embedding using Resnet50 Model\n",
        "#4) Count the number of Images for lung_n\n",
        "\n",
        "for filename in os.listdir('/content/Cancer Image Classification/lung_colon_image_set/lung_image_sets/lung_n'):\n",
        "    if filename.endswith('.jpeg'):\n",
        "        img_path = os.path.join('/content/Cancer Image Classification/lung_colon_image_set/lung_image_sets/lung_n', filename)\n",
        "        with open(img_path, 'rb') as f:\n",
        "            im_bytes = f.read()\n",
        "            img = Image.open(BytesIO(im_bytes))\n",
        "            img_t = preprocess(img)\n",
        "            batch_t = torch.unsqueeze(img_t, 0)\n",
        "            with torch.no_grad():\n",
        "                features = model(batch_t)\n",
        "                # Flatten the features for easier use\n",
        "                embeddings = torch.flatten(features)\n",
        "                input_data[input_count] = embeddings\n",
        "                input_count += 1\n",
        "#input_count = 20000 (15048 -> 19999)"
      ],
      "metadata": {
        "id": "oZiumHLx5Sgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Load the Images from lung_scc Folder\n",
        "#2) Preprocess Each Image using the proprocess objects defined earlier\n",
        "#3) Create Image embedding using Resnet50 Model\n",
        "#4) Count the number of Images for lung_scc\n",
        "\n",
        "for filename in os.listdir('/content/Cancer Image Classification/lung_colon_image_set/lung_image_sets/lung_scc'):\n",
        "    if filename.endswith('.jpeg'):\n",
        "        img_path = os.path.join('/content/Cancer Image Classification/lung_colon_image_set/lung_image_sets/lung_scc', filename)\n",
        "        with open(img_path, 'rb') as f:\n",
        "            im_bytes = f.read()\n",
        "            img = Image.open(BytesIO(im_bytes))\n",
        "            img_t = preprocess(img)\n",
        "            batch_t = torch.unsqueeze(img_t, 0)\n",
        "            with torch.no_grad():\n",
        "                features = model(batch_t)\n",
        "                # Flatten the features for easier use\n",
        "                embeddings = torch.flatten(features)\n",
        "                input_data[input_count] = embeddings\n",
        "                input_count += 1\n",
        "#input_count = 25000 (20000 -> 24999)"
      ],
      "metadata": {
        "id": "3qWoWjy65piM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tensor to a file\n",
        "torch.save(input_data, '/content/drive/My Drive/tensor.pth')"
      ],
      "metadata": {
        "id": "rhfgJMDs50vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Size of Preporcesses Image: torch.Size([3, 224, 224])\n",
        "2.   Size of Batch: torch.Size([1, 3, 224, 224]) -> Size of Unsqueezed Image\n",
        "3.   Size of Features: torch.Size([1, 1000]) -> Model Output\n",
        "4.   Size of Embeddings: torch.Size([1000]) -> Final Embedding Size"
      ],
      "metadata": {
        "id": "PJ2rq2vl6C5p"
      }
    }
  ]
}