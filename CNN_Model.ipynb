{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NSxieUJ6v9y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sklearn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tensor back from the file\n",
        "X = torch.load('/content/drive/My Drive/tensor.pth')\n",
        "\n",
        "# Check the loaded tensor\n",
        "print(X.size())\n",
        "\n",
        "#torch.Size([25000, 1000])"
      ],
      "metadata": {
        "id": "4NQ5JRWB6123"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_label = torch.empty(25000, 1)\n",
        "label_count = 0\n",
        "\n",
        "#Colon_aca = 0  -> 5048\n",
        "#Colon_n = 1    -> 10048\n",
        "#lung_aca = 2   -> 15048\n",
        "#lung_n = 3     -> 20000\n",
        "#lung_scc = 4   -> 25000\n",
        "\n",
        "\n",
        "for i in range(25000):\n",
        "    if i < 5048:\n",
        "        input_label[i] = 0\n",
        "    elif i >= 5048 and i < 10048:\n",
        "        input_label[i] = 1\n",
        "    elif i >= 10048 and i < 15048:\n",
        "        input_label[i] = 2\n",
        "    elif i >= 15048 and i < 20000:\n",
        "        input_label[i] = 3\n",
        "    else:\n",
        "        input_label[i] = 4"
      ],
      "metadata": {
        "id": "Mu4yrrlg617j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, input_label, test_size = 0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "ck1FKVj-61-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a Custom Dataset class to define a Dataloader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "1s2QJj1e7CTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader instances for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "O4OLJSrF7Ca2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN MODEL ARCHITECTURE DEFINITION\n",
        "class CancerHistModel(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv_block_3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(240, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.conv_block_3(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Ri6QN7jB7ClI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model = CancerHistModel(input_shape=1, hidden_units=16, output_shape=5)\n",
        "Model\n",
        "\n",
        "# CancerHistModel(\n",
        "#   (conv_block_1): Sequential(\n",
        "#     (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#     (1): ReLU()\n",
        "#     (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#     (3): ReLU()\n",
        "#     (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "#   )\n",
        "#   (conv_block_2): Sequential(\n",
        "#     (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#     (1): ReLU()\n",
        "#     (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#     (3): ReLU()\n",
        "#     (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "#   )\n",
        "#   (conv_block_3): Sequential(\n",
        "#     (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#     (1): ReLU()\n",
        "#     (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#     (3): ReLU()\n",
        "#     (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "#   )\n",
        "#   (classifier): Sequential(\n",
        "#     (0): Flatten(start_dim=1, end_dim=-1)\n",
        "#     (1): Linear(in_features=240, out_features=5, bias=True)\n",
        "#   )\n",
        "# )"
      ],
      "metadata": {
        "id": "SA2PVJBA7dcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true,y_pred):\n",
        "  correct = torch.eq(y_true,y_pred).sum().item()\n",
        "  return correct\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = Model.parameters(),lr=0.1)"
      ],
      "metadata": {
        "id": "9M_plYjc7dlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "def print_train_time(start: float,\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "  total_time = end=start\n",
        "  print(f\"toral time: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "bv5FxvIh7duj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creting a training loop and training a model on Batches of data\n",
        "\n",
        "from tqdm.auto import tqdm #to get he progress bar\n",
        "import time\n",
        "\n",
        "#Set the seed the start the times\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = time.time\n",
        "\n",
        "#Set number of epochs\n",
        "epochs = 5\n",
        "\n",
        "#Create training and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch} \\n---\")\n",
        "\n",
        "  #Training\n",
        "  train_loss = 0\n",
        "  #Add a loop to loop through training batches\n",
        "  for batch, (X,y) in enumerate (train_loader):\n",
        "    Model.train()\n",
        "\n",
        "    X = X.view(32,1,40,25)\n",
        "\n",
        "    #Forward Pass\n",
        "    y_pred = Model(X)\n",
        "\n",
        "    # y = y.argmax(dim=1)\n",
        "    #Calculate loss\n",
        "    loss = loss_fn(y_pred,y.squeeze().long())\n",
        "    # print(f\"this is my calculates loss: {loss}\")\n",
        "    train_loss += loss\n",
        "\n",
        "    #Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    #Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    #print out what's happeing\n",
        "    if batch%100 == 0:\n",
        "      print(f\"Lookd at { batch * len(X)}/{len(train_loader.dataset)} samples.\")\n",
        "\n",
        "  #Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(train_loader)\n",
        "\n",
        "  #Testing\n",
        "  test_loss, test_acc = 0,0\n",
        "  Model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X,y in test_loader:\n",
        "      if X.shape[0] != 32:\n",
        "        continue\n",
        "\n",
        "      X = X.view(32,1,40,25)\n",
        "      test_pred = Model(X)\n",
        "      # y = y.argmax(dim=1)\n",
        "      test_loss += loss_fn(test_pred,y.squeeze().long())\n",
        "\n",
        "      test_acc += accuracy_fn(y_true=y,y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    #Calculate the test loss average per batch\n",
        "    test_loss /= (len(test_loader)-1)\n",
        "    #calculate test acc average per batch\n",
        "    num_test_batches = len(test_loader) - 1\n",
        "    num_test_ele = num_test_batches * 32 * 8\n",
        "    test_acc_val = test_acc / num_test_ele\n",
        "    test_acc_val *= 100\n",
        "\n",
        "  print(f\"\\nTrain Loss {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc_val:.3f}\")\n",
        "\n",
        "train_time_end_on_cpu = time.time()"
      ],
      "metadata": {
        "id": "nYoSQd8e8Fff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1\n",
        "---\n",
        "Lookd at 0/20000 samples.\n",
        "Lookd at 3200/20000 samples.\n",
        "Lookd at 6400/20000 samples.\n",
        "Lookd at 9600/20000 samples.\n",
        "Lookd at 12800/20000 samples.\n",
        "Lookd at 16000/20000 samples.\n",
        "Lookd at 19200/20000 samples.\n",
        "\n",
        "Train Loss 0.6506 | Test loss: 0.1796, Test acc: 88.962\n",
        "Epoch: 2\n",
        "---\n",
        "Lookd at 0/20000 samples.\n",
        "Lookd at 3200/20000 samples.\n",
        "Lookd at 6400/20000 samples.\n",
        "Lookd at 9600/20000 samples.\n",
        "Lookd at 12800/20000 samples.\n",
        "Lookd at 16000/20000 samples.\n",
        "Lookd at 19200/20000 samples.\n",
        "\n",
        "Train Loss 0.1815 | Test loss: 0.1567, Test acc: 89.220\n",
        "Epoch: 3\n",
        "---\n",
        "Lookd at 0/20000 samples.\n",
        "Lookd at 3200/20000 samples.\n",
        "Lookd at 6400/20000 samples.\n",
        "Lookd at 9600/20000 samples.\n",
        "Lookd at 12800/20000 samples.\n",
        "Lookd at 16000/20000 samples.\n",
        "Lookd at 19200/20000 samples.\n",
        "\n",
        "Train Loss 0.1450 | Test loss: 0.1083, Test acc: 89.438\n",
        "Epoch: 4\n",
        "---\n",
        "Lookd at 0/20000 samples.\n",
        "Lookd at 3200/20000 samples.\n",
        "Lookd at 6400/20000 samples.\n",
        "Lookd at 9600/20000 samples.\n",
        "Lookd at 12800/20000 samples.\n",
        "Lookd at 16000/20000 samples.\n",
        "Lookd at 19200/20000 samples.\n",
        "\n",
        "Train Loss 0.1258 | Test loss: 0.1496, Test acc: 89.032\n",
        "Epoch: 5\n",
        "---\n",
        "Lookd at 0/20000 samples.\n",
        "Lookd at 3200/20000 samples.\n",
        "Lookd at 6400/20000 samples.\n",
        "Lookd at 9600/20000 samples.\n",
        "Lookd at 12800/20000 samples.\n",
        "Lookd at 16000/20000 samples.\n",
        "Lookd at 19200/20000 samples.\n",
        "\n",
        "Train Loss 0.1165 | Test loss: 0.1085, Test acc: 89.501\n",
        "\n",
        "\n",
        "As it can be Observed here, both train and test loss decrease as we train our model. It can also be observed that our model accuracy increases with more Training with final accuracy reaching upto 89.5%.\n",
        "\n",
        "Important thing to highlight here is that both train and test loss are comparable in the end, hence we can say that our model is neither overfitting or underfitting."
      ],
      "metadata": {
        "id": "qVr9QCzZ8LOA"
      }
    }
  ]
}